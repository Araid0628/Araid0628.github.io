<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker批量删除REPOSITORY、TAG为none的镜像]]></title>
    <url>%2F2019%2F12%2F24%2Fdocker-delete-none%2F</url>
    <content type="text"><![CDATA[第一次构建镜像时生成的镜像ID为A，此镜像会被标记为TAG-A 第二次构建镜像时生成的镜像ID为B，此镜像的标签仍为TAG-A Docker会移除ID为A的标签，此时A镜像就变成了dangling images，在docker images中显示为&lt;/none>:&lt;/none> 官方给出的解决方案是：1docker image prune 还可以这样做：1docker images|grep none|awk ‘&#123;print $3&#125;’|xargs docker rmi grep: 查找images中符合none的字符串awk: 输出xargs: 给命令传递参数]]></content>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode 21 合并两个有序链表]]></title>
    <url>%2F2019%2F11%2F15%2Fleetcode-21%2F</url>
    <content type="text"><![CDATA[第一次在python里用到链表这个数据结构，和cpp里很像，但还是要借用cpp中指针的概念。题目的重点是合并两个链表的过程。非递归的答案：12345678910111213141516171819202122232425class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def mergeTwoLists(self, l1, l2): newList = ListNode(-1) if l1 is None and l2 is None: return None pre = newList while l1 and l2: if l1.val &gt; l2.val: pre.next = l2 l2 = l2.next else: pre.next = l1 l1 = l1.next pre = pre.next if l1 is not None: pre.next = l1 if l2 is not None: pre.next = l2 return newList.next 如果把第一个判断条件的and改为or会有一个测试用例无法通过： Testcase[][0] Answer[] Expected Answer[0]]]></content>
      <tags>
        <tag>Python</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine的优点与比较]]></title>
    <url>%2F2019%2F11%2F06%2FGMP%2F</url>
    <content type="text"><![CDATA[goroutine的优点： goroutine可以在用户空间调度，避免了内核态和用户态的切换导致的成本。 goroutine是语言原生支持的，提供了非常简洁的语法，屏蔽了大部分底层实现。 goroutine有更小的初始栈空间，goroutine消耗的资源更小，允许用户创建成千上万的实例。 Java ThreadJava里的Thread使用的是线程模型的一对一模型，每一个轻量级进程都对应着一个内核线程。Java使用的线程调度方式主要是抢占式调度。 扩展点 用户态和内核态内核态：进程能访问所有的内存空间和对象用户态：进程能访问的内存空间和对象受到限制用户态切换到内核态有三种方法：(1) 系统调用，实质是中断机制，如Linux中的INT 80H(2) 异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。如：缺页异常(3) 中断：软中断硬中断 goroutine中的栈管理：]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[goroutine]]></title>
    <url>%2F2019%2F10%2F22%2Fgoroutine%2F</url>
    <content type="text"><![CDATA[并发和并行并发意味着程序在任意时刻都是同时运行的，在规定时间内多个请求都得到执行和处理，分时操作系统就是一种并发设计。并行意味着程序在单位时间内是同时运行的，在任一粒度时间内都具备同时执行的能力，最简单的例子就是多台机器并行处理。 协程 coroutine协程是一种用户态的轻量级线程，操作执行者是用户自身程序。协程的执行是非抢占式多任务处理，由协程主动交出控制权，多个协程可能在一个或多个线程上面运行。不同语言对协程的支持： C++: Boost.Coroutine Java: 不支持 Python: async def(e.g. Discord中向频道用户发送消息)goroutine操作系统本身具备并发处理能力，但是进程切换的代价过高。所以Go语言在语言层面再构筑一级调度，将并发的粒度降低（编译器/解释器/虚拟机层面）。goroutine是建立在线程之上的轻量级的抽象，它允许我们以非常低的代价在同一个地址空间中并行地执行多个函数或者方法。在golang中创建一个goroutine只要使用go+匿名/有名函数的形式就可以启动。 12345678910111213141516171819202122package main import ( "fmt" "runtime" "time" ) func main() &#123; for i := 0; i &lt; 1000; i++&#123; go func(i int) &#123; for&#123; fmt.Printf("Hello from" + "goroutine from %d\n", i) // 设置或查询可以并发执行的goroutine数目 fmt.Println("GOMAXPROCS=", runtime.GOMAXPROCS(0)) &#125; &#125;(i) &#125; time.Sleep(time.Minute) &#125; goroutine特性 go的执行是非阻塞的，不会等待。 go后面函数的返回值会被忽略。 调度器不能保证多个goroutine的执行次序，所有的goroutine是平等地调度和执行的。 Go程序会单独为main函数创建一个goroutine]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言的错误处理：panic, recocer, error, error wrapping]]></title>
    <url>%2F2019%2F10%2F17%2FpanicRecoverDefer%2F</url>
    <content type="text"><![CDATA[panic和recoverpanicfunc panic(v interface{})引发panic有两种情况，一种是程序主动调用panic函数（e.g. panic(xxx), panic的参数是一个空接口类型，任意类型的变量都可以传递给panic），另一种是程序产生运行时错误。发生panic后，程序会从调用panic的函数位置或发生panic的地方立即返回，逐层向上执行函数的defer语句，然后逐层打印函数调用堆栈，直到被recover捕获或运行到最外层函数而退出。12345678910111213141516package main import "fmt" func tryDefer()&#123; for i := 0; i &lt; 100; i++&#123; defer fmt.Println(i) if i == 30 &#123; panic("print end") &#125; &#125; &#125; func main() &#123; tryDefer() &#125; 当i循环到30时，系统会从30倒序进行Println操作，最后打印“panic: print end”。 recoverfunc recover() interface{}recover用来捕获panic，阻止panic继续向上传递。recover()只有在defer后面的函数体内被直接调用才能捕获panic终止异常，否则返回nil，异常继续向外传递。1234defer func()&#123; fmt.Println("function body") recover()&#125; 记得主动在程序的中使用recover()拦截运行时错误。 error12345678910111213141516// Package errors implements functions to manipulate errors. package errors // New returns an error that formats as the given text. func New(text string) error &#123; return &amp;errorString&#123;text&#125; &#125; // errorString is a trivial implementation of error. type errorString struct &#123; s string &#125;func (e *errorString) Error() string &#123; return e.s &#125; error的底层实现非常简单，所以功能也比较弱，只能返回一个字符串。Golang 1.13中引入了一个新功能Error Wrapping。]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu服务器配置(tensorflow+cuda+cudnn+显卡驱动)]]></title>
    <url>%2F2019%2F07%2F26%2Fgpu-server%2F</url>
    <content type="text"><![CDATA[服务器我们组有两台服务器一直在某个小房间里吃灰，光靠工作站里的两块1080Ti算力确实不够。某天和IT一起把线接好了可以启动了，但是一看开发环境还是两年多前的，各种软件版本都比较旧了。两块Tesla P100在里面积了不少灰，曾经想把他拆下来装在台式机上后来发现台式机电源不是全模组的根本没那个供电的接口，还是放回去吧。 GPU Compute Capability Tesla P100 6.0 GeForce GTX 1080 Ti 6.1 Retrieved from https://developer.nvidia.com/cuda-gpus三四万一张的P100计算能力怎么还没有1080Ti好使呢?这个计算能力应该只是在深度学习领域。Tesla系列的专业显卡相比Geforce显卡拥有更高的计算精度，对于航空航天、药物研发等领域提供双精度（FP64）进行更加精准的计算。Tesla的显存颗粒更贵，支持高精度计算不会在读写存储时出现误差。建议立刻更新到Quadro RTX 8000或者Tesla T4（没funding） 环境显卡驱动 禁用nouveau驱动Ubuntu系统集成的显卡驱动程序是nouveau，我们需要先将nouveau从linux内核卸载掉才能安装NVIDIA官方驱动。我们的服务器是纯命令行，直接执行最后一步即可。（1） 修改blacklist.conf 文件属性blacklist.conf查看属性• ll /etc/modprobe.d/blacklist.conf修改属性• sudo chmod 666 /etc/modprobe.d/blacklist.conf用vi编辑器打开• sudo vi /etc/modprobe.d/blacklist.conf• 在文件末尾添加如下几行：blacklist vga16fbblacklist nouveaublacklist rivafbblacklist rivatvblacklist nvidiafb（2）修改并保存文件后，记得把文件属性复原：• sudo chmod 644 /etc/modprobe.d/blacklist.conf更新一下内核• sudo update-initramfs -u修改后需要重启系统• reboot (重启)重启系统确认nouveau是否已经被屏蔽掉，使用lsmod命令查看，lsmod命令用于显示已经加载到内核中的模块的状态信息• lsmod | grep nouveau若已屏蔽没有任何信息 手动安装驱动提前载nvidia官网上下好了对应版本的驱动，直接用jupyter notebook传到服务器上就可以。• 进入nvidia官网 https://www.geforce.cn/drivers下载对应显卡的驱动程序，下载后的文件格式为run。• 删除原有的NVIDIA驱动程序(若没有安装忽略)• sudo apt-get remove –purge nvidia*• 进入命令行桌面给驱动文件增加可执行权限• sudo chmod a+x NVIDIA-Linux-x86_64-418.74.run（cd到文件所在文件夹。）安装• sudo sh ./NVIDIA-Linux-x86_64-418.74.run.run –no-opengl-files 检查输入nvidia-smi即可检查驱动是否安装成功。 安装CUDA 进入官网下载对应版本(https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal)2.运行指令需要cd到包含该文件的文件夹目录下根据官网提示指令安装即可• sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.168-418.67_1.0-1_amd64.deb• sudo apt-key add /var/cuda-repo-ubuntu1804-10-1-local-10.1.168/7fa2af80.pub• sudo apt-get update• sudo apt-get install cuda 安装cuDnn 进入官网注册并下载对应的cudnn版本。(https://developer.nvidia.com/rdp/cudnn-download) 下载完成后解压文件tar -zxvf cudnn-10.1-linux-x64-v7.5.1.10.tgz 解压完成将相应的文件拷贝到对应的CUDA目录下即可（要cd到含cudnn文件的目录下）• sudo cp cuda/include/cudnn.h /usr/local/cuda/include/• sudo cp cuda/lib64/libcudnn /usr/local/cuda/lib64/• sudo chmod a+r /usr/local/cuda/include/cudnn.h• sudo chmod a+r /usr/local/cuda/lib64/libcudnn 编辑环境变量• sudo gedit ~/.bashrc将cuda的环境变量加到打开的文件最后• export LD_LIBRARY_PATH=”$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64”• export CUDA_HOME=/usr/local/cuda• export PATH=“CUDAHOME/bin:PATH”保存后，终端输入• source ~/.bashrc]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>配置</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SNKRS API变量解析]]></title>
    <url>%2F2019%2F07%2F14%2Fnikeapi%2F</url>
    <content type="text"><![CDATA[引言&emsp;&emsp;想做这个项目的原因是因为之前买的snkrs bot附带的Discord监控到期了，不能再提供各种球鞋上新监控。刚好有朋友在做球鞋定制和公众号，未来是希望把Discord和微信小程序集成在公众号下。第一步选择先做Discord端，考虑到Discord只要把后端和Discord Bot完成就行，刚好也能先熟悉一下Nike这个接口返回的各种信息。但是Discord因为某些原因被墙了，中国的开发者很少，花了很多时间研究国外开发者写的文档，进度还是比较理想的。希望暑假后能尽快开始微信小程序的开发，这才是重点！ SNKRS APIhttps://api.nike.com/snkrs/content/v1/?country=CN&amp;language=zh-Hans&amp;orderBy=lastUpdated&amp;offset=0URL其实比较容易拿到，用Wireshark对手机访问SNKRS APP时进行抓包操作就可以。当offset=0时，API会返回第1-50双球鞋的数据，offset=50时，API会返回第51-100双球鞋的数据，以此类推。通过totalRecords可以看到中国区SNKRS上一共有1468双球鞋。1&#123;"country":"CN","locale":"zh-Hans_CN","channel":"snkrs","totalRecords":1468,"threads":[&#123;"id":"50307c93-04ef-4720-a5ba-d65f53595d5e","threadId":"50307c93-04ef-4720-a5ba-d65f53595d5e","interestId":"5c94b7ea-2a8a-3c70-97a1-e394f65c77f8","name":"M2K Tekno","lastUpdatedDate":"2019-07-14T04:30:25.997000","lastUpdatedTime":"2019-07-14T04:30:25.997000","publishedDate":"2019-06-11T02:02:43.000000","restricted":false,"feed":"d879aa85-698c-41a1-b066-a8ecdf6b8a9c","title":"Electric Volt","subtitle":"M2K Tekno","seoSlug":"m2k-tekno-electric-volt","seoTitle":"Nike M2K Tekno 'Electric Volt' 发布日期","seoDescription":"浏览并选购 Nike M2K Tekno 'Electric Volt'。抢先了解运动鞋发布新动态。","tags":["SPORTSWEAR","M2K TEKNO GEL","VOLT"],"imageUrl":"https://c.static-nike.com/a/images/t_prod_ss/w_750,c_limit/itzc9whni67vdgvffry5/nike-m2k-tekno-electric-volt-release-date.jpg","squareImageUrl":"https://c.static-nike.com/a/images/t_prod_ss/w_750,c_limit/itzc9whni67vdgvffry5/nike-m2k-tekno-electric-volt-release-date.jpg","portraitImageUrl":"https://c.static-nike.com/a/images/t_prod_ps/w_750,c_limit/itzc9whni67vdgvffry5/nike-m2k-tekno-electric-volt-release-date.jpg","relations":[&#123;"name":"RELATED","threads":["295a2e30-e337-4d16-a7dc-6ed37efdf6cb","54b800b9-2457-4f2d-973e-0f77502cb19f"]&#125;],"product":&#123;"id":"a13e8a5d-eed0-5a05-bac7-96913b8528eb","interestId":"5c94b7ea-2a8a-3c70-97a1-e394f65c77f8","style":"CI5749","colorCode":"777","globalPid":"12697014","title":"Nike M2K Tekno","subtitle":"男子运动鞋","imageUrl":"https://secure-images.nike.com/is/image/DotCom/CI5749_777","genders":["MEN"],"quantityLimit":10,"merchStatus":"ACTIVE","colorDescription":"荧光黄/荧光黄/荧光黄","available":true,"publishType":"FLOW","productType":"FOOTWEAR","upcoming":true,"price":&#123;"msrp":799,"fullRetailPrice":799,"currentRetailPrice":799,"onSale":false&#125;,"startSellDate":"2019-06-12T01:00:00.000000","skus":[&#123;"id":"af4e2d70-66b7-5b8f-b1cd-6f25118412ec","localizedSize":"38.5","localizedSizePrefix":null,"nikeSize":"6","available":false&#125;,&#123;"id":"a6198452-cea8-52b1-b4e8-9b7330ce1738","localizedSize":"39","localizedSizePrefix":null,"nikeSize":"6.5","available":false&#125;,&#123;"id":"0ffd3b93-6f33-56c4-9150-e8399d48a925","localizedSize":"40","localizedSizePrefix":null,"nikeSize":"7","available":true&#125;,&#123;"id":"2288c661-37d8-58d2-9420-073e84d94d42","localizedSize":"40.5","localizedSizePrefix":null,"nikeSize":"7.5","available":true&#125;,&#123;"id":"61a76fb0-9f4b-5415-831d-79a7d95aacd0","localizedSize":"41","localizedSizePrefix":null,"nikeSize":"8","available":true&#125;,&#123;"id":"ce9e61be-1a77-5612-9da5-d3d7c7e1b474","localizedSize":"42","localizedSizePrefix":null,"nikeSize":"8.5","available":false&#125;,&#123;"id":"05d73ea3-ba82-5d5e-8272-c3c855f4eee0","localizedSize":"42.5","localizedSizePrefix":null,"nikeSize":"9","available":true&#125;,&#123;"id":"d14e0a6b-2e5a-5950-a616-19354ae727d0","localizedSize":"43","localizedSizePrefix":null,"nikeSize":"9.5","available":true&#125;,&#123;"id":"509c830c-d527-5836-9047-998a504e044e","localizedSize":"44","localizedSizePrefix":null,"nikeSize":"10","available":true&#125;,&#123;"id":"1949e1fb-c5d5-503b-9d57-13a1b4722c10","localizedSize":"44.5","localizedSizePrefix":null,"nikeSize":"10.5","available":true&#125;,&#123;"id":"d47d9cf2-08d7-5d8a-8621-924fae3bc22a","localizedSize":"45","localizedSizePrefix":null,"nikeSize":"11","available":true&#125;,&#123;"id":"9f7ddf99-b7b8-5768-9311-7aed805526e9","localizedSize":"46","localizedSizePrefix":null,"nikeSize":"12","available":true&#125;,&#123;"id":"9ea19342-cf46-50cd-a9f9-5dcbb45cd2fa","localizedSize":"47.5","localizedSizePrefix":null,"nikeSize":"13","available":false&#125;],"expireDate":"3000-01-01T20:00:00.000000"&#125; 变量解析country: CN 中国区totalRecords: 1468 SNKRS APP中记录总量threads: 球鞋的所有信息，一个很长的list，中间存储的是dict。以第一双鞋为例：是一双AJ13，这双鞋在threads中的dict中有一些重要的信息： idAPI中标记一双球鞋的编号，主键 name球鞋名称 restricted是否受限，有些新球鞋是锁住的 imageUrl球鞋图片，Discord中推送会用到 selectionEngineLEO和DAN，2分钟和15分钟抽签模式，现在已经没有先到先得的FLOW模式了 skus尺码]]></content>
      <categories>
        <category>SnkrsMonitor</category>
      </categories>
      <tags>
        <tag>JSON</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个面试问题（字节跳动）]]></title>
    <url>%2F2019%2F04%2F14%2Finterview1%2F</url>
    <content type="text"><![CDATA[问题如果现在有十三个特征，把其中一个特征复制了再加进去变成第十四个特征，对权重有什么影响？ 实验面试结束忘记问答案了，自己做了个实验看不出任何规律。在Dataframe中复制一列已有的特征1data_train['ByteDance'] = data_train['avgprice'] 画出特征权重直方图1xgb.plot_importance(model) f_{12} = 625, f_{13} = 132 f_{12} = 643\widehat{f_{12}} < f_{12}+f_{13}？？？]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to XGboost]]></title>
    <url>%2F2019%2F04%2F11%2Fxgboostppt%2F</url>
    <content type="text"><![CDATA[实习的时候主管布置的任务：Pls. help to prepare some material to us a training in your area. AI in picture recognition. Model algorithm use case in different area.至今还没机会讲，就当帮自己从底层梳理了一遍XGboost原理。]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实训-利用LSTM进行房价走势预测]]></title>
    <url>%2F2019%2F04%2F10%2Flstm%2F</url>
    <content type="text"><![CDATA[LSTM网络Long short-term memory，即LSTM。在传统的RNN中，所有的RNN都具有一种重复神经网络模块的链接形式，例如一个tanh层。LSTM的重复模块有一个不同的结构。]]></content>
      <tags>
        <tag>深度学习</tag>
        <tag>项目实训</tag>
        <tag>Python</tag>
        <tag>LSTM</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习知识点总结]]></title>
    <url>%2F2019%2F04%2F03%2FIntroduction%2F</url>
    <content type="text"><![CDATA[机器学习知识点总结整理了一些机器学习的知识点，在公司用TexStudio写的，持续更新中。如果pdf无法显示请打开Google Drive的分享链接。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实训-利用XGboost进行新楼盘价格预测]]></title>
    <url>%2F2019%2F03%2F23%2Fxgboost%2F</url>
    <content type="text"><![CDATA[前言本来这个需求不在列表中，受到Kaggle比赛：Kaggle Boston Housing(https://www.kaggle.com/c/boston-housing)的启发，加入了这个功能。因为数据全部靠自己收集，特征也远不如比赛中的数据集多，最终预测的效果我觉得虽然误差比较大，但是也有一定的参考价值。 数据清洗和特征工程从数据库中抽取了一部分好量化的上海市数据作为基本信息，在此基础之上，引入了该楼盘周边1km范围内地铁站数量、医院数量、学校数量、商场数量（数据来自百度API，可以参见另一篇blog）和该楼盘所在区域的区域均价。从Figure1中可以看到从某壳网上爬取下来的数据中包含很多汉字、符号，还有缺失值需要填补，另外还有格式不统一的情况，所以在数据清洗上花了很多时间。对数据中的字符型数据转换：12345678# 数据预处理，填充缺失值以及特征中含有字符的转换为数值型# "price","propertyType","landscapingRatio","siteArea","floorAreaRatio","buildingArea","yearofpropertyRights",# "numPlan","parkingRatio","propertycosts","parkingSpace","hospital","metro","school","mall","id"# 住宅：1 写字楼：2 别墅：3 商业：4train.loc[train["propertyType"] == "住宅", "propertyType"] = 1train.loc[train["propertyType"] == "写字楼", "propertyType"] = 2train.loc[train["propertyType"] == "别墅", "propertyType"] = 3train.loc[train["propertyType"] == "商业", "propertyType"] = 4 对部分特征统一数据格式后，缺失值用均值进行填补：123456789101112131415train['landscapingRatio'] = train['landscapingRatio'].fillna(train.groupby('propertyType')['landscapingRatio'].transform('mean'))train['siteArea'] = train['siteArea'].fillna(train.groupby('propertyType')['siteArea'].transform('mean'))train['floorAreaRatio'] = train['floorAreaRatio'].fillna(train.groupby('propertyType')['floorAreaRatio'].transform('mean'))train['buildingArea'] = train['buildingArea'].fillna(train.groupby('propertyType')['buildingArea'].transform('mean'))train = train.fillna(0)train['yearofpropertyRights'] = train['yearofpropertyRights'].astype(float)train['numPlan'] = train['numPlan'].astype(int)train['parkingRatio'] = train['parkingRatio'].astype(float)train['propertycosts'] = train['propertycosts'].astype(float)train['parkingSpace'] = train['parkingSpace'].astype(int)train['yearofpropertyRights'] = train['yearofpropertyRights'].fillna(train.groupby('propertyType')['yearofpropertyRights'].transform('mean'))train['numPlan'] = train['numPlan'].fillna(train.groupby('propertyType')['numPlan'].transform('mean'))train['parkingRatio'] = train['parkingRatio'].fillna(train.groupby('propertyType')['parkingRatio'].transform('mean'))train['propertycosts'] = train['propertycosts'].fillna(train.groupby('propertyType')['propertycosts'].transform('mean'))train['parkingSpace'] = train['parkingSpace'].fillna(train.groupby('propertyType')['parkingSpace'].transform('mean')) XGboost简介后期会把在公司做的seminar的ppt放上来 建模Step 1读取数据集，为了增强模型的泛化能力，筛掉了单价超过50000元/平方米的数据，实验结果证明效果对二三线城市的预测效果比较好，但在像上海、北京这样房价过高的城市表现一般。12345678dataset_train = 'house_trainset2.csv'data_train = pd.read_csv(dataset_train)data_train = data_train[data_train['price'] &lt;= 49999]scaler = MinMaxScaler(feature_range=(0, 1))pd.set_option('display.width', None)X = data_train.drop(['id', 'price', 'Unnamed: 0', 'numPlan'], axis=1)X = scaler.fit_transform(X)y = data_train.price 随机拆分训练集和测试集后，fit到模型中，模型的参数调整用了Sklearn中的GridSearchCV,它存在的意义就是自动调参，只要把参数输进去，就能给出最优化的结果和参数。但是这个方法适合于小数据集，一旦数据的量级上去了，很难得出结果。数据量比较大的时候可以使用一个快速调优的方法——坐标下降。它其实是一种贪心算法：拿当前对模型影响最大的参数调优，直到最优化；再拿下一个影响最大的参数调优，如此下去，直到所有的参数调整完毕。12345678910X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)cv_params = &#123;'n_estimators': [400, 500, 600, 700, 800]&#125;other_params = &#123;'learning_rate': 0.1, 'n_estimators': 400, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1&#125;optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=5, verbose=1, n_jobs=5)optimized_GBM.fit(X_train, y_train)evalute_result = optimized_GBM.grid_scores_print('每轮迭代运行结果:&#123;0&#125;'.format(evalute_result))print('参数的最佳取值：&#123;0&#125;'.format(optimized_GBM.best_params_))print('最佳模型得分:&#123;0&#125;'.format(optimized_GBM.best_score_))print(xg_reg.feature_importances_) 预测结果]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>项目实训</tag>
        <tag>Python</tag>
        <tag>XGboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实训-第二周总结]]></title>
    <url>%2F2019%2F03%2F11%2Fweek2%2F</url>
    <content type="text"><![CDATA[前言第二周结束项目的大部分简单需求已经实现，超过了预期的速度。两个组员在隔壁新国大实习，我周五也要去Bosch实习了，下周要加快进度。 定位网站的定位是面向购买新房的投资者或者有住房提升需求的买房者的功能性网站，数据分析＞房源展示。 已完成的需求 按用户输入城市名称，爬取和显示城市不同小区，不同区域的房价，并用柱状图显示看了周一的模拟产品发布以后，发现隔壁组的UI做的很好看，扁平化设计，准备在产品功能实现后再调整。 按年份，月份，显示和分析某区域房价走向和趋势收集到的数据的粒度到月，住建局有到天的成交数据，但是很多特征都没有就没有采集。ECharts确实很好用，提供了一些直观、易用的交互方式以方便对所展现数据的再加工。待完成的需求预测模型的训练已经完成，为了不和其他组撞车，准备加入两个预测模块。1.新楼盘开盘价预测（博文会在第三周整理后发布）输入：用户输入新楼盘的一些信息，城市+小区名称（在后台调用百度地图API返回小区周边信息数据）、绿化率、容积率、车位比等数据输出：该新楼盘的开盘价格(基于XgBoost)图中的蓝色点是训练集中上海新楼盘的真实信息，但是训练的结果和Kaggle中波士顿房价预测那题的结果相差甚远，我想有几个原因：第一是上海主城区房价过高，但是新楼盘少，新城区房价较低，新楼盘多，这些离群点让模型产生了较大误差；第二，Kaggle比赛的数据集很全，虽然说有缺失值和部分错误值，但是用Pandas完全可以解决这些小错误，我们这次预测的数据集60%来自链家，40%是自己收集的，在专业性上相差较多，导致没有比赛数据集所呈现的统计规律。2.房价走势预测（博文会在第三周整理后发布）输入：数据库中某城市粒度位月的历史房价输出：未来一个月的房价趋势（基于LSTM）]]></content>
      <categories>
        <category>项目实训</category>
      </categories>
      <tags>
        <tag>项目实训</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas处理数据时replace方法失效]]></title>
    <url>%2F2019%2F03%2F05%2Fweek123%2F</url>
    <content type="text"><![CDATA[BUG复现1train.replace('%', '', inplace=True) 使用replace函数的目的是：在清洗房价数据时替换掉不需要的单位，如“%”，“元/m²/月”等。但在使用函数时发现无法生效，查了网上相关信息后试了多种方法也无法解决这个问题，如先取列之后再进行replace。1train["landscapingRatio"].replace('%', '', inplace=True) 原因推测dataframe中每个列中值的dtypes不同，导致无法replace。 解决方法使用map()和lambda函数1train["landscapingRatio"] = train["landscapingRatio"].map(lambda x: x.replace('%', '')) map() 会根据提供的函数对指定序列做映射。第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。]]></content>
      <categories>
        <category>项目实训</category>
      </categories>
      <tags>
        <tag>房价预测</tag>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[项目实训第一周总结]]></title>
    <url>%2F2019%2F03%2F03%2Fhousepricingweek1%2F</url>
    <content type="text"><![CDATA[前言寒假前已经定好了这次项目实训的题目，放假的时候群里关于房价数据爬虫的讨论随着一句“过完年再说”戛然而止，然而第一周已经结束了才空出时间来写总结。 项目需求城市房价分析系统技术类别：Web，网络爬虫，数据分析基本功能：通过爬取链家网，我爱我家等中介机构，以及城市建设局网站数据，实现对指定城市的房价进行排序和分析，并结合一定规律进行预测难易程度：中等需求实现： 按用户输入城市名称，爬取和显示城市不同小区，不同区域的房价，并用柱状图显示 按年份，月份，显示和分析某区域房价走向和趋势 对比相同价格下不同城市的房产品牌 对比不同城市房价走向，并进行归类分析 实现房价波动的简单预测 需求理解1.项目是什么？一个Web，可以向用户展示粒度从市—行政区—具体楼盘的统计数据，可以根据用户提供的新楼盘数据做预测。2.项目怎么做？Java Web(Spring bot, vue.js, Echarts)Python(bs4, sklearn, pandas等)MySQL百度API（POI相关信息获取）预测模型XgBoost + LR/DT, 是否采用Deep Learning视项目进度决定 开发模式Scrum敏捷开发模式，lm担任产品经理，ltz担任技术主管，我是master。作为master在项目中要负责管理上的事务很多，经理日报，进度控制，每日组会，团建等等，对人际沟通能力是个很好的提升。组员都很自觉，分工我和lm负责数据和预测部分，另外三位负责Web。 第一周进度 数据部分：爬虫完成，爬取了某壳网上一线城市和二线城市的数据 Web部分：通过百度地图的控件可以在地图上显示城市和行政区的统计数据，校对经纬度花了挺久。 预测部分：数据清洗中，第二周预测模块应该能实现。]]></content>
      <categories>
        <category>项目实训</category>
      </categories>
      <tags>
        <tag>房价预测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度地图API解析]]></title>
    <url>%2F2019%2F02%2F28%2FbaiduAPI%2F</url>
    <content type="text"><![CDATA[功能简介百度地图Web服务API为开发者提供http/https接口，即开发者通过http/https形式发起检索请求，获取返回json或xml格式的检索数据。用户可以基于此开发JavaScript、C#、C++、Java等语言的地图应用。http://lbsyun.baidu.com/index.php?title=webapi文档非常全面，可以获得很多信息，包括经纬度，经纬度周边任意距离任意类型场所信息，如医院、地铁站等，为预测做准备。以正/逆地理编码服务为例。 Python如何解析API返回的json所用到的库12import jsonfrom urllib.request import urlopen, quote 获得API传回的json信息1showLocation&amp;&amp;showLocation(&#123;"status":0,"result":&#123;"location":&#123;"lng":120.75204667293927,"lat":31.27628928466111&#125;,"precise":1,"confidence":80,"comprehension":100,"level":"道路"&#125;&#125;) 1234567891011def getlnglat(address): url = 'http://api.map.baidu.com/geocoder/v2/' output = 'json' ak = '你的密钥' add = quote(address) # 由于本文城市变量为中文，为防止乱码，先用quote进行编码 uri = url + '?' + 'address=' + add + '&amp;output=' + output + '&amp;ak=' + ak print(uri) req = urlopen(uri) res = req.read().decode() # 将其他编码的字符串解码成unicode responseInfo = json.loads(res) # 对json数据进行解析 return responseInfo 取得所需的信息123add = "江苏省苏州市苏州工业园区林泉街399号东南大学软件学院" # 你所要查询的地址lat = getlnglat(add)['result']['location']['lat'] # 获得纬度lng = getlnglat(add)['result']['location']['lng'] # 获得经度 结果lat = 31.27628928466111lng = 120.75204667293927]]></content>
      <categories>
        <category>项目实训</category>
      </categories>
      <tags>
        <tag>房价预测</tag>
        <tag>Python</tag>
        <tag>百度</tag>
        <tag>API</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯估计与最大似然估计]]></title>
    <url>%2F2018%2F12%2F02%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E4%B8%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[贝叶斯估计：估计参数$\theta$是随机变量，根据观测数据对参数的分布进行估计，考虑先验分布。 最大似然估计：参数$\theta$是未知的，根据真实数据通过最大化似然函数$\mathop{\arg\max}_{\theta}L(\theta|D)$来估计$\theta$的值 最大似然估计 贝叶斯估计 计算复杂度 微分 多重积分 先验信息的信任程度 不准确 准确 例如$p(x丨\theta)$ 与初始假设一致 与初始假设不一致]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯</tag>
        <tag>最大似然估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法导论4.5-主方法]]></title>
    <url>%2F2018%2F11%2F12%2F%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA4-5-%E4%B8%BB%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[算法导论练习4.5-1对下列递归式，使用主方法求出渐进紧缺界: $T(n) = 2T(n/4) + 1$$T(n) = 2T(n/4) + \sqrt{n}$$T(n) = 2T(n/4) + n$$T(n) = 2T(n/4) + n^2$Answer： $\Theta(n^{\log_4{2}}) = \Theta(\sqrt{n})$$\Theta(n^{\log_4{2}}\lg{n}) = \Theta(\sqrt{n}\lg{n})$$\Theta(n)$$\Theta(n^2)$]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>算法导论</tag>
        <tag>递归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《机器学习实战》SVM补充内容]]></title>
    <url>%2F2018%2F10%2F21%2F%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%E3%80%8BSVM%E8%A1%A5%E5%85%85%E5%86%85%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[对于《机器学习实战》一书SVM章节程序清单6-2 简化版SVM算法中部分代码的数学原理补充 if(labelMat[i] != labelMat[j]): L = max(0, alphas[j]-alphas[i]) H = min(C, C+alphas[j]-alphas[i]) else: L = max(0, alphas[j]+alphas[i]-C) H = min(C, C+alphas[j]-alphas[i]) 这段代码在《机器学习实战》书中只说明了保证alpha在0与C之间，根据约束条件$0\leq\alpha_i\leq C\;i=1,2$，实际上是单变量的最优化问题。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Hexo进行Latex公式渲染测试]]></title>
    <url>%2F2018%2F10%2F12%2FLatex%E6%B8%B2%E6%9F%93%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[安装插件首先需要安装Mathjax插件，MathJax是一款运行在浏览器中的开源的数学符号渲染引擎，使用MathJax可以方便的在浏览器中显示数学公式，不需要使用图片。目前，MathJax可以解析Latex、MathML和ASCIIMathML的标记语言。(Wiki) npm install hexo-math –save 更换渲染引擎接着要更换Hexo的markdown渲染引擎 npm uninstall hexo-renderer-marked –save npm install hexo-renderer-kramed –save 更改配置文件进入到主题目录，找到_config.yml文件，把mathjax默认的值改为true # MathJax Support mathjax: enable: true per_page: true #cdn: //cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML 写博客在每次编写需要使用LaTeX渲染的博客文章中，需要在Front-matter里打开mathjax开关 title: test abbrlink: 7717490f date: 2018-10-12 10:06:12 tags: mathjax: true 测试 $T(n) = \Theta(n)$ $T(n) = \Theta(n)$]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘 Week1作业]]></title>
    <url>%2F2018%2F09%2F19%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98WEEK1%E4%BD%9C%E4%B8%9A%2F</url>
    <content type="text"><![CDATA[第一周作业要求2组人工搜集中国科学院院士、中国工程院院士的名单，在百度自动获取院士个人主页url，编写爬虫采集院士人类学、学位、研究方向和国家科技奖数据，设计数据库表，保存数据。 小组分工CHY：科学院院士信息抓取ZBC：数据到MySQLme：工程院院士信息抓取 Step1 网站分析中国工程院院士名单：http://www.cae.cn/cae/html/main/col48/column_48_1.html 可以看到所有学部的院士信息已经汇总在一个网页中，后续编写爬虫抓取个人主页url很方便。 Step2 抓取url在编写爬虫时用到了XPath，XPath 是一门在 XML 文档中查找信息的语言。XPath 用于在 XML 文档中通过元素和属性进行导航。 XPath 使用路径表达式在 XML 文档中进行导航 XPath 包含一个标准函数库 XPath 是 XSLT 中的主要元素 XPath 是一个 W3C 标准在这里通过XPath的语法分别抓取li标签下的herf的内容和text的内容，以获得院士个人主页的url和院士的姓名，存储在list中。 def getURL(): #抓取工程院院士的URL和姓名 page = urllib.request.urlopen('http://www.cae.cn/cae/html/main/col48/column_48_1.html') html = page.read() urlList = [] sel = Selector(text=html, type="html") urlList = sel.xpath('//li[re:test(@class, "name_list")]//@href').extract() nameList = sel.xpath('//li[re:test(@class, "name_list")]//a/text()').extract() #print(url) i = 0 for i in range(len(urlList)): urlList[i] = "http://www.cae.cn" + urlList[i] return urlList, nameList #print(url) Step3 抓取个人信息接着仍使用XPath语法从urlList中依次爬取院士主页的个人信息。 infoPage = urllib.request.urlopen(academyURL) infoHTML = infoPage.read() infoSelect = Selector(text=infoHTML, type="html") info = infoSelect.xpath('//div[@class="intro"]/p/text()').extract()[0] return info Step4 信息提取可以发现院士信息页中大部分信息描述具有某些规律，比如描述院士在某个专业方面的成就时的描述会采用“金属材料及粉末冶金专家”、“耳鼻咽喉学专家”等以“专家”、“学家”结尾的格式，可以通过正则表达式和findall()函数找出对应的描述。 pattern = re.compile(r'(\d{4}年(?:\d{1,2}月)?(?:\d{0,2}日)?)(?:(?:出生)|(?:生，)|(?:生于)|(?:出生于))') birthday = pattern.findall(c) pattern2 = re.compile(r'^.*?(?:(?:学家)|(?:专家))') direction = pattern2.findall(c) pattern3 = re.compile(r'(?:获)[^(?:。)]+(?:奖)') award = pattern3.findall(c) pattern4 = re.compile(r'博士|硕士|学士') degree = pattern4.findall(c)]]></content>
      <categories>
        <category>作业</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>数据挖掘</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用插件和IDM下载百度云盘]]></title>
    <url>%2F2018%2F09%2F10%2F%E4%BD%BF%E7%94%A8%E6%8F%92%E4%BB%B6%E5%92%8CIDM%E4%B8%8B%E8%BD%BD%E7%99%BE%E5%BA%A6%E4%BA%91%E7%9B%98%2F</url>
    <content type="text"><![CDATA[背景在某度云上保存了很多课程，但因为某度对于网盘速度的限制，只能寻找破解版某度云盘下载文件。听说破解版最近不好使了，在网上查找了很多方法，发现只有利用Chrome的插件和IDM才能达到最理想的下载效果。 软件 Chrome浏览器 IDM下载器（(Integrated Data Multiplexer） 扩展程序 BaiduPan Explorer 步骤 安装上述软件 使用插件抓取链接 使用IDM下载即可]]></content>
      <categories>
        <category>技巧</category>
      </categories>
      <tags>
        <tag>IDM</tag>
        <tag>插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，Hexo]]></title>
    <url>%2F2018%2F09%2F09%2F%E4%BD%A0%E5%A5%BD%EF%BC%8CHexo%2F</url>
    <content type="text"></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
</search>
